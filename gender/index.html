
<head>
  <link rel="stylesheet" href="croppie.css" />
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"> </script>
  <script src="croppie.js"></script> 
</head> 
<body>
  <h1><p style="color:purple">// Genderflow //</p> </h1>
  
  
  <!-- ...... -->
  Face classifier trained on top of VGG16 with Adience face data set (~2.5k faces for each male and female faces).
  There is lots of room for improvement -- this is just first quick shot.
  </p>
  This model his highly biased and has reveals unpleasant gender stereotypes, as found in the training data...
  
  Select an image with a face, ideally centered and frontal.<br>
  System does not crop automatically -- classification works best when face fills most of the image.<br>
  </p>

  Step 1: <input id="step1" type="file" onchange="selectImage()"></p>
  Step 2: <button id="step2" onclick="loadNewImage()">load new image</button></p>
  Step 3: <button id="step3" onclick="classifyImage()">Click here to classify</button></p>
  
  <img id="myImage" src="" alt="" width="150" height="150">
  

  <h1><p style="color:purple">>> Result</p> </h1>
  
  <div id="myResultImage"></div>                                 
  <div id="myResultLabel"></div>                                 
  <div id="myResultConf"></div>
  <div id="myResultPred"></div> 
  <div id="myResultDetails"></div>
  </p>
  


<!--
var el = document.getElementById('myImage')
var cropper = new Croppie(el, {
    viewport: { width: 150, height: 150 },
    boundary: { width: 300, height: 300 },
    showZoomer: true,
    enableOrientation: true
});
-->
<script>
var cropper;
  

function selectImage(){
       //cropper.destroy()
       var preview = document.querySelector('img'); //selects the query named img
       var file    = document.querySelector('input[type=file]').files[0]; //sames as here
       var reader  = new FileReader();
       reader.onloadend = function () {
           preview.src = reader.result;
       }
       if (file) {
           reader.readAsDataURL(file); //reads the data as a URL
       } else {
           preview.src = "";
       }
       document.getElementById("step1").disable=true
    }
  
function loadNewImage() {
  var el = document.getElementById('myImage')
  cropper = new Croppie(el, {
      enableExif: true,
      viewport: { width: 150, height: 150 },
      boundary: { width: 300, height: 300 },
      showZoomer: true,
      enableOrientation: true
  });  
  document.getElementById("step2").disable=true
}
  



  

// normalize into some sort of confidence for given class
function norm(v) {
    if (v>0.5) {
        return (v-0.5) / (1.0 - 0.5)
    } else {
        return 1 - ((v-0) / (0.5 - 0))
    }
}
  

  
async function classifyImage() {
 document.getElementById("myResultImage").innerHTML = ""; 
 document.getElementById("myResultLabel").innerHTML = "thinking..."; 
 document.getElementById("myResultConf").innerHTML = "loading image..."; 
 document.getElementById("myResultPred").innerHTML = ""; 
 
 var croppedImagaData = await cropper.result('canvas')   
 var img = new Image();
 img.src = croppedImagaData;
 cropper.destroy()
 document.getElementById('myImage').appendChild(img)
  
 document.getElementById("myResultConf").innerHTML = "loading model..."; 

 var model = await tf.loadModel('faces_to_gender_v2.h5/model.json')      
 
 document.getElementById("myResultConf").innerHTML = "classifying..."; 
  
 tf.tidy(() => {
   
   // var img = document.getElementById("myImage"); 
   var example =tf.fromPixels(img);
   example = tf.image.resizeBilinear(example, [150, 150], align_corners=true);   
   example = example.toFloat().div(tf.scalar(255));
   example = example.reshape([1, 150, 150, 3]); 
   // example.print()
   document.getElementById("myResultDetails").innerHTML = "[image tensor:" + example + "]";
   console.log(example)
  
   var pred = model.predict(example);
 
   // decision
   var conf = norm(pred.dataSync())
   var label = 'x'
   if (pred.dataSync() >= 0.59) {
      label='MALE'
      document.getElementById("myResultLabel").innerHTML = label.toString();
      document.getElementById("myResultConf").innerHTML = "confidence: " + parseFloat(conf).toFixed(2);
      document.getElementById("myResultPred").innerHTML = "[raw softmax:" + parseFloat(pred.dataSync()).toFixed(4) + "]";
     
  } else if (pred.dataSync() <= 0.41) {
      label='FEMALE'
      document.getElementById("myResultLabel").innerHTML = label.toString();
      document.getElementById("myResultConf").innerHTML = "confidence: " + parseFloat(conf).toFixed(2);
      document.getElementById("myResultPred").innerHTML = "[raw softmax:" + parseFloat(pred.dataSync()).toFixed(4) + "]";
    
  } else {
      label='I have no idea...'
      document.getElementById("myResultLabel").innerHTML = "class: " + label.toString();
      document.getElementById("myResultConf").innerHTML = "";
      document.getElementById("myResultPred").innerHTML = "[raw softmax:" + parseFloat(pred.dataSync()).toFixed(4) + "]";
    
  }
 });
 model.dispose();
  
 document.getElementById("step1").disable=false
 document.getElementById("step2").disable=false
}
  
  
</script>
</body>
